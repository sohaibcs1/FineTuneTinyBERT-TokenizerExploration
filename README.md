
# ğŸ“ The Impact of Tokenization on TinyBERT Performance Across Text Classification Tasks

This repository demonstrates the use of text classification techniques by using three popular tokenization methods: **BPE (Byte Pair Encoding)**, **WordPiece**, and **SentencePiece**. In addition to tokenization, the project includes fine-tuning of a lightweight BERT model, Tiny BERT, to achieve efficient and accurate classification on datasets like IMDb, AG News, and Yelp. This combination of tokenization strategies and model fine-tuning showcases the impact of preprocessing and optimization on NLP tasks.

---

## ğŸ“– Table of Contents

1. [ğŸ¯ Introduction](#ğŸ¯-introduction)
2. [âœ¨ Features](#âœ¨-features)
3. [âš™ï¸ Installation](#âš™ï¸-installation)
4. [ğŸš€ Usage](#ğŸš€-usage)
5. [ğŸ”§ Tokenizer Techniques](#ğŸ”§-tokenizer-techniques)
6. [ğŸ§  Models and Parameters](#ğŸ§ -models-and-parameters)
7. [ğŸ“Š Datasets](#ğŸ“Š-datasets)
8. [ğŸ“‚ Folder Structure](#ğŸ“‚-folder-structure)
9. [ğŸ¤ Contributing](#ğŸ¤-contributing)
10. [ğŸ“œ License](#ğŸ“œ-license)
11. [ğŸ‘ Acknowledgments](#ğŸ‘-acknowledgments)

---

## ğŸ¯ Introduction

Tokenization is a crucial step in **Natural Language Processing (NLP)**. This repository explores three popular tokenization techniques:

- **BPE Tokenizer**: Splits text into subwords using frequency-based merges.
- **WordPiece Tokenizer**: A subword tokenization technique used in models like BERT.
- **SentencePiece Tokenizer**: Implements subword tokenization with language model independence.

Each tokenizer is evaluated on text classification tasks using datasets such as **IMDb**, **AG News**, and **Yelp**.

---

## âœ¨ Features

- ğŸ› ï¸ Implementation of **BPE**, **WordPiece**, and **SentencePiece** tokenizers.
- ğŸ“š Support for multiple datasets (**IMDb**, **AG News**, and **Yelp**).
- ğŸ“Š Interactive demonstrations using **Jupyter Notebooks**.
- ğŸ” Visualizations to understand tokenization results.

---

## âš™ï¸ Installation

### Prerequisites

- ğŸ Python 3.8+
- ğŸ“’ Jupyter Notebook or Jupyter Lab
- ğŸ“¦ Required libraries listed in `requirements.txt`

### Setup

1. Clone the repository:
   ```bash
   git clone https://github.com/your-username/tokenizer-techniques.git
   cd tokenizer-techniques
   ```
---

## ğŸš€ Usage

### Step 1: Open the Jupyter Notebook

```bash
jupyter notebook
```

### Step 2: Choose a Dataset and Tokenizer Notebook

Navigate to the appropriate dataset folder (`imdb`, `ag_news`, `yelp`) and select a notebook for a specific tokenizer (e.g., `BPE_Tokenizer.ipynb`).

### Step 3: Run the Notebook

Follow the step-by-step execution in the notebook to preprocess text, apply tokenization, and visualize results.

---

## ğŸ”§ Tokenizer Techniques

### **1. Byte Pair Encoding (BPE) Tokenizer** ğŸ§©

- **Description**: Splits text into smaller subwords based on frequent pairings of characters or subwords.
- **Parameters**: 
  - Vocabulary size: `30,000`
  - Minimum frequency: `2`
  - Special tokens: `<pad>`, `<cls>`, `<sep>`

### **2. WordPiece Tokenizer** ğŸ§©

- **Description**: Used in models like BERT, WordPiece learns a compact vocabulary and creates tokens based on frequency thresholds.

### **3. SentencePiece Tokenizer** ğŸ§©

- **Description**: A language-independent tokenizer that converts text into sequences of subwords.

---

## ğŸ§  Models and Parameters

### **BERT-Tiny Model** ğŸ§ 
- **Architecture**: A lightweight version of BERT (`prajjwal1/bert-tiny`).
- **Sequence Classification**: Supports binary classification tasks.
- **Hyperparameters**:
  - Learning rate: `2e-6`
  - Weight decay: `2e-4`
  - Epochs: `5`
  - Batch size: `2`
  - Optimizer: AdamW

---

## ğŸ“Š Datasets

### **1. IMDb** ğŸ¥
- **Type**: Sentiment Analysis
- **Description**: A dataset of movie reviews with binary sentiment labels (positive/negative).

### **2. AG News** ğŸ“°
- **Type**: Topic Classification
- **Description**: Contains news articles categorized into four classes: **World**, **Sports**, **Business**, and **Science/Technology**.

### **3. Yelp** ğŸ´
- **Type**: Sentiment Analysis
- **Description**: Yelp reviews labeled for positive and negative sentiments.

Preprocessed datasets and tokenizer artifacts (e.g., `vocab.json`, `merges.txt`) are included in their respective folders.

---

## ğŸ“‚ Folder Structure

```
Notebooks_extracted/
â”œâ”€â”€ imdb/
â”‚   â”œâ”€â”€ BPE_Tokenizer.ipynb
â”‚   â”œâ”€â”€ WordPiece_Tokenizer.ipynb
â”‚   â”œâ”€â”€ SentencePiece_Tokenizer.ipynb
â”œâ”€â”€ ag_news/
â”‚   â”œâ”€â”€ BPE_Tokenizer.ipynb
â”‚   â”œâ”€â”€ WordPiece_Tokenizer.ipynb
â”‚   â”œâ”€â”€ SentencePiece_Tokenizer.ipynb
â”‚   â””â”€â”€ bpe_tokenizer/
â”‚       â”œâ”€â”€ vocab.json
â”‚       â”œâ”€â”€ merges.txt
â”œâ”€â”€ yelp/
â”‚   â”œâ”€â”€ BPE_Tokenizer.ipynb
â”‚   â”œâ”€â”€ WordPiece_Tokenizer.ipynb
â”‚   â”œâ”€â”€ SentencePiece_Tokenizer.ipynb
```

---

## ğŸ¤ Contributing

We welcome contributions! To contribute:

1. Fork the repository.
2. Create a feature branch.
3. Submit a pull request with your changes.

---

## ğŸ“œ License

This project is licensed under the **MIT License**.

---

## ğŸ‘ Acknowledgments

Special thanks to:

- The creators of the **IMDb**, **AG News**, and **Yelp** datasets.

